{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Word-Level** **Tokenization**"
      ],
      "metadata": {
        "id": "dxKLAxKi6jWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download necessary resources for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"We love NLP!\"\n",
        "\n",
        "# Word-level tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Output the result\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjbZ4oFA6h9W",
        "outputId": "5a3904c0-4f41-4c80-f986-c2b2a17f66cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'love', 'NLP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Character-Level Tokeniation**"
      ],
      "metadata": {
        "id": "Cz5MLN736Ymk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"We Love NLP!\"\n",
        "\n",
        "# Character-level tokenization\n",
        "tokens = list(text)\n",
        "\n",
        "# Output the result\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwMO9ylL2mPu",
        "outputId": "34e89108-f257-4e10-f27c-147a2d18d8e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['W', 'e', ' ', 'L', 'o', 'v', 'e', ' ', 'N', 'L', 'P', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BytePair encoding** **Tokenization**"
      ],
      "metadata": {
        "id": "vJwhRycw6RXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def get_vocab(text):\n",
        "    # Create initial vocabulary with word frequency counts\n",
        "    words = text.split()\n",
        "    vocab = Counter(words)\n",
        "    return vocab\n",
        "\n",
        "def get_pairs(vocab):\n",
        "    # Count pairs of characters in the vocabulary\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word + \"</w>\"  # Add end of word symbol\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "-GVIlnQO3Tml"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair, vocab):\n",
        "    # Merge the most frequent pair in the vocabulary\n",
        "    new_vocab = {}\n",
        "    bigram = ''.join(pair)\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(''.join(pair), bigram)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "def bpe_tokenization(text, num_merges):\n",
        "    vocab = get_vocab(text)\n",
        "\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_pairs(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best_pair = pairs.most_common(1)[0][0]\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "Y9VUZlPK3-QV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Eat Eat Sleep Repeat Repeat repeat\"\n",
        "\n",
        "# Perform BPE tokenization\n",
        "num_merges = 20  # Number of merges to perform\n",
        "tokenized_vocab = bpe_tokenization(text, num_merges)\n",
        "\n",
        "# Output the result\n",
        "print(tokenized_vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR1R03cw4BPc",
        "outputId": "f4ad840b-d3b6-4f60-872b-9503d29a20a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Eat': 2, 'Sleep': 1, 'Repeat': 2, 'repeat': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZLUDD5t4ErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WordPiece** **Tokenization**"
      ],
      "metadata": {
        "id": "EKvaE1jO5gd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_vocab(text):\n",
        "    # Create a vocabulary of words from the input text\n",
        "    words = text.split()\n",
        "    vocab = defaultdict(int)\n",
        "    for word in words:\n",
        "        vocab[word] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_subwords(vocab, max_vocab_size=100):\n",
        "    # Create a set of subwords starting from characters\n",
        "    subwords = set()\n",
        "    for word in vocab:\n",
        "        subwords.add(word)  # Add the full word\n",
        "        for char in word:\n",
        "            subwords.add(char)  # Add individual characters\n",
        "\n",
        "    # Limit vocabulary size\n",
        "    return sorted(list(subwords))[:max_vocab_size]"
      ],
      "metadata": {
        "id": "t4xRMUOl5jE_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordpiece_tokenize(text, vocab):\n",
        "    # Tokenize the input text using WordPiece algorithm\n",
        "    tokens = []\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            tokens.append(word)\n",
        "            continue\n",
        "\n",
        "        subword = ''\n",
        "        start = 0\n",
        "        while start < len(word):\n",
        "            # Check for longest matching subword\n",
        "            matched = False\n",
        "            for end in range(len(word), start, -1):\n",
        "                subword_candidate = word[start:end]\n",
        "                if subword_candidate in vocab:\n",
        "                    tokens.append(subword_candidate)\n",
        "                    start = end\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                tokens.append(word[start])  # Add the character if no match\n",
        "                start += 1\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "8mc2iPMB5oDn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Eat Eat Sleep Repeat Repeat repeat.\"\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = create_vocab(text)\n",
        "\n",
        "# Get subwords for WordPiece tokenization\n",
        "subwords = get_subwords(vocab)\n",
        "\n",
        "# Perform WordPiece tokenization\n",
        "tokenized_output = wordpiece_tokenize(text, subwords)\n",
        "\n",
        "# Output the result\n",
        "print(tokenized_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4AZQaGK5p1M",
        "outputId": "a37d5699-9236-49db-cb8c-34804c9a2d91"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eat', 'Eat', 'Sleep', 'Repeat', 'Repeat', 'repeat.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pD9d8zw95tdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MDYbGFRU61A2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}